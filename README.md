# wikipedia-scraper
BeCode_Challenge_2 scraping political leaders of each country

## Description
This project aims to create a scraper that retrieves data about political leaders from an API and extracts additional information from their Wikipedia pages. The extracted data is then saved into a JSON file for further processing.

## Usage
1. Clone the repository to your local machine.
2. Install the required dependencies by running `pip install -r requirements.txt`.
3. Execute the `main.py` script to initiate the scraping process and generate the JSON file.
4. Follow the prompts and instructions provided by the script.

## Installation
To install the necessary dependencies, use the following command:

## Mission Objectives
In this project, we guide you through the process of:
1. Creating a self-contained development environment.
2. Retrieving information from an API.
3. Scraping a website without an API using BeautifulSoup.
4. Saving the extracted data to a JSON file.
5. Implementing proper exception handling and error logging.

## Learning Objectives
- Setting up a virtual environment using `venv`.
- Making HTTP requests to an external API using `requests`.
- Extracting text from HTML using `BeautifulSoup`.
- Implementing proper exception handling.
- Working with JSON data.
- Implementing object-oriented programming principles.
- Using regular expressions to clean text data.
- (_Optional_) Utilizing multiprocessing to improve performance.

## The Mission
Create a scraper that retrieves information about political leaders from the [Country Leaders API](https://country-leaders.onrender.com/docs) and extracts additional details from their Wikipedia pages. The extracted data, including the first paragraph of each leader's Wikipedia page, should be saved into a JSON file.

### Must-have features
- Implement a functional `main.py` script that interacts with the API and generates a JSON file.
- Implement custom exceptions for proper error handling.
- Provide clear installation and execution instructions in the README.

### Nice-to-have features
- Utilize `Session()` from the `requests` library for optimized HTTP requests.
- Add an option to save the output as CSV instead of JSON.
- Enhance performance using multiprocessing.

## Contributors
Solo assignment by [Your Name]

## Timeline
This project was completed in two days.

## Visuals
_No visuals provided._

## Anything else useful
Feel free to explore and extend the functionality of the scraper as needed. If you encounter any issues or have suggestions for improvements, please let us know!
